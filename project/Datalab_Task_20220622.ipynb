{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixt Data Science Lab - Test Task for Data Scientist Job Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this test task you will have an opportunity to demonstrate your skills of a Data Scientist from various angles - processing data, analyzing and vizalizing it, finding insights, applying predictive techniques and explaining your reasoning about it.\n",
    "\n",
    "The task is based around a bike sharing dataset openly available at UCI Machine Learning Repository [1].\n",
    "\n",
    "Please go through the steps below, build up the necessary code and comment on your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Loading and Environment Preparation\n",
    "\n",
    "**Tasks:**\n",
    "1. Prepare a Python 3 virtual environment (with virtualenv command). requirements.txt output of pip freeze command should be included as part of your submission.\n",
    "2. Load the data from UCI Repository and put it into the same folder with the notebook. The link to it is https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset . Here is an available mirror in case the above website is down: https://data.world/uci/bike-sharing-dataset\n",
    "3. We split the data into two parts. One dataset containing the last 30 days and one dataset with the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#436EEE\"> Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.getcwd()\n",
    "\n",
    "# subfolders\n",
    "print(os.listdir(\"data\"))\n",
    "print(os.listdir(\"output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sys.prefix)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import sweetviz as sv\n",
    "from scipy.stats import scoreatpercentile\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn import preprocessing, metrics, linear_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, StratifiedKFold,  cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config display options\n",
    "pd.options.display.max_colwidth = 10000\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Display all outputs in Jupyter Notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# I want pandas to show all columns and up to * rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Environment for images\n",
    "# This sets reasonable defaults for font size for\n",
    "# a figure that will go in a notebook\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# Set the font to be serif, rather than sans\n",
    "sns.set(font='serif')\n",
    "\n",
    "# Make the background white, and specify the\n",
    "# specific font family\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#436EEE\"> Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read raw training data\n",
    "df_all = pd.read_csv('data/day.csv')\n",
    "df_hour_all = pd.read_csv('data/hour.csv')\n",
    "\n",
    "# split dataset\n",
    "df_last30 = df_all.tail(30) # use to test data in unseen data\n",
    "df = df_all.iloc[:-30, :] # use to train data\n",
    "\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data Processing and Analysis\n",
    "\n",
    "**Tasks:**\n",
    "1. Perform all needed steps to load and clean the data. Please comment the major steps of your code. \n",
    "2. Visualise rentals of bikes per day. \n",
    "3. Assume that each bike has exactly maximum 12 rentals per day.\n",
    "    * Find the maximum number of bicycles `nmax` that was needed in any one day. <span style=\"color:#436EEE\"> answer here\n",
    "    * Find the 95%-percentile of bicycles `n95` that was needed in any one day. <span style=\"color:#436EEE\"> answer here\n",
    "5. Visualize the distribution of the covered days depending on the number of available bicycles (e.g. `nmax` bicycles would cover 100% of days, `n95` covers 95%, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#436EEE\"> Dataset characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv\n",
    "\t\n",
    "\t- instant: record index\n",
    "\t- dteday : date\n",
    "\t- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "\t- yr : year (0: 2011, 1:2012)\n",
    "\t- mnth : month ( 1 to 12)\n",
    "\t- hr : hour (0 to 23)\n",
    "\t- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "\t- weekday : day of the week\n",
    "\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\t+ weathersit : \n",
    "\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\t- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "\t- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "\t- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "\t- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "\t- casual: count of casual users\n",
    "\t- registered: count of registered users\n",
    "\t- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#436EEE\"> Storytelling\n",
    "### <span style=\"color:#436EEE\"> EDA - Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.info()\n",
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Report about total dataset, with target feature\n",
    "eda_report = sv.analyze([df_all,'Bike Rentals'], 'cnt')\n",
    "eda_report.show_html('output/analyze_dataset.html')\n",
    "eda_report.show_notebook(layout='widescreen', w=1500, h=700, scale=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments / reasoning:**\n",
    "\n",
    "- I observe a strong positive correlation between registered and total rents\n",
    "- Casual users have a moderate correlation with total rents\n",
    "- We have less total rents during Spring months (more rentals during Summer/Autumn) and we observe much more rentals during holidays, since it is really conducive to ride bike in that season and moment of the week. Therefore June, July, August and September has relatively higher demand for bicycle.\n",
    "- Users prefer to rent bikes when windspeed is < 0.2 (low windspeed), however there's an interesting pike when windspeed is around 0.45\n",
    "- Humidity only has a negotive influence for bike rentals when is < 0.8 (people not comfortable with sweating too much...)\n",
    "- When the temperature is higher we observe higher rentals \n",
    "- There's a great increase in bike rentals from 2011 to 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Report comparison about split datasets, with target feature\n",
    "eda_report_comparison = sv.compare([df_all, 'training data'], [df_last30, 'last 30 days'], 'cnt')\n",
    "eda_report_comparison.show_html('output/analyze_dataset_comparison.html')\n",
    "eda_report_comparison.show_notebook(layout='widescreen', w=1500, h=700, scale=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments / reasoning:**\n",
    "\n",
    "- Different behaviour bike rentals during weekdays, last 30 days dataset has more rentals on Wednesdays and very low on Mondays (if  0 = Monday, 1 = Tuesday, 2 = Wednesday, 3 = Thursday, 4 = Friday, 5 = Saturday, 6 = Sunday)\n",
    "- From last 30 days dataset I observe that windspeed impacted more rentals when between 0.2 and 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#436EEE\"> EDA - Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:#436EEE\"> Remove one of the temperature variables, because they are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_all.rename(columns={'instant':'id','dteday':'datetime','yr':'year','mnth':'month','weathersit':'weather_condition',\n",
    "                       'temp':'temperature', 'atemp':'feel_temperature', 'hum':'humidity','cnt':'total_count'},inplace=True)\n",
    "df_all.head()\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all['datetime']=pd.to_datetime(df_all.datetime)\n",
    "df_all['season']=df_all.season.astype('category')\n",
    "df_all['year']=df_all.year.astype('category')\n",
    "df_all['month']=df_all.month.astype('category')\n",
    "df_all['holiday']=df_all.holiday.astype('category')\n",
    "df_all['weekday']=df_all.weekday.astype('category')\n",
    "df_all['workingday']=df_all.workingday.astype('category')\n",
    "df_all['weather_condition']=df_all.weather_condition.astype('category')\n",
    "\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('df_all shape : ' + str(df_all.shape))\n",
    "\n",
    "# Split dataframe by numerical and categorical columns\n",
    "num_df = df_all.select_dtypes(include = ['int64', 'float64'])\n",
    "cat_df = df_all.select_dtypes(include = ['object', 'bool'])\n",
    "\n",
    "# Get list of columns with missing values\n",
    "missing_num = num_df.isnull().sum()\n",
    "columns_with_missing_num = missing_num[missing_num > 0]\n",
    "print(\"**These are the NUMERIC columns with missing values:**\\n{} \\n\"\\\n",
    "      .format(columns_with_missing_num))\n",
    "\n",
    "# Get list of columns with missing values\n",
    "missing_cat = cat_df.isnull().sum()\n",
    "columns_with_missing_cat = (missing_cat[(missing_cat > 0) & (missing_cat < len(df_all))])\n",
    "print(\"**These are the CATEGORICAL columns with missing values:**\\n{} \\n\"\\\n",
    "      .format(columns_with_missing_cat))\n",
    "\n",
    "columns_with_all_missing_num = missing_num[missing_num == len(df_all)]\n",
    "columns_with_all_missing_num = list(columns_with_all_missing_num.index)\n",
    "print(\"**These are the NUMERICAL columns with ALL missing values:**\\n{} \\n\"\\\n",
    "      .format(columns_with_all_missing_num))\n",
    "\n",
    "columns_with_all_missing_cat = missing_cat[missing_cat == len(df_all)]\n",
    "columns_with_all_missing_cat = list(columns_with_all_missing_cat.index)\n",
    "print(\"**These are the CATEGORICAL columns with ALL missing values:**\\n{}\"\\\n",
    "      .format(columns_with_all_missing_cat))\n",
    "\n",
    "df_all.drop(columns_with_all_missing_num, axis = 1, inplace = True)\n",
    "df_all.drop(columns_with_all_missing_cat, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Handling High Cardinality in Categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Considering cardinality_threshold\n",
    "cardinality_threshold = 10\n",
    "\n",
    "# Get list of columns with their cardinality - don't want to consider numeric columns\n",
    "categorical_columns = list(df_all.select_dtypes(exclude=[np.number]).columns)\n",
    "cardinality = df_all[categorical_columns].apply(pd.Series.nunique)\n",
    "columns_too_high_cardinality = list(cardinality[cardinality > cardinality_threshold].index)\n",
    "print(\"There are {} columns with high cardinality. Threshold: {} categories.\"\\\n",
    "      .format(len(columns_too_high_cardinality), cardinality_threshold))\n",
    "columns_too_high_cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remove ID & unnecessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ID_variables = ['id']\n",
    "\n",
    "df_all.drop(ID_variables, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casual & registered – These variables cannot be predicted\n",
    "no_value_variables = ['casual', 'registered']\n",
    "\n",
    "df_all.drop(no_value_variables, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remove constant columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get list of columns with constant value\n",
    "columns_constant = list(df_all.columns[df_all.nunique() <= 1])\n",
    "print(\"There are {} columns with constant values\".format(len(columns_constant)))\n",
    "columns_constant\n",
    "\n",
    "df_all.drop(columns_constant, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remove perfect correlated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = df_all.select_dtypes(exclude=[np.object]).corr().abs()\n",
    "\n",
    "# select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# find features with correlation > 0.95\n",
    "columns_perfect_correlation = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(\"There are {} columns that are perfectly correlated with other columns: {} \"\\\n",
    "          .format(len(columns_perfect_correlation), columns_perfect_correlation))\n",
    "\n",
    "columns_perfect_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I prefer to drop 'temp' the column atemp is more appropriate for modelling purposes, from human perspective\n",
    "df_all.drop('temperature', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (5, 3) )\n",
    "\n",
    "# boxplot for total_count outliers\n",
    "sns.boxplot(data = df_all[['total_count']])\n",
    "ax.set_title('total_count outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot box plot of categorical variables\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.subplot(3,3,1)\n",
    "sns.boxplot(x = 'season', y = 'total_count', data = df_all)\n",
    "plt.subplot(3,3,2)\n",
    "sns.boxplot(x = 'year', y = 'total_count', data = df_all)\n",
    "plt.subplot(3,3,3)\n",
    "sns.boxplot(x = 'month', y = 'total_count', data = df_all)\n",
    "plt.subplot(3,3,4)\n",
    "sns.boxplot(x = 'holiday', y = 'total_count', data = df_all)\n",
    "plt.subplot(3,3,5)\n",
    "sns.boxplot(x = 'weekday', y = 'total_count', data = df_all)\n",
    "plt.subplot(3,3,6)\n",
    "sns.boxplot(x = 'workingday', y = 'total_count', data = df_all)\n",
    "plt.subplot(3,3,7)\n",
    "sns.boxplot(x = 'weather_condition', y = 'total_count', data = df_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot box plot of continuous variables\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.subplot(2,3,1)\n",
    "plt.boxplot(df_all[\"feel_temperature\"])\n",
    "plt.subplot(2,3,3)\n",
    "plt.boxplot(df_all[\"humidity\"])\n",
    "plt.subplot(2,3,4)\n",
    "plt.boxplot(df_all[\"windspeed\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (7, 5))\n",
    "\n",
    "# zoom in for outliers regarding windspeed & humidity features\n",
    "sns.boxplot(data = df_all[['windspeed','humidity']])\n",
    "ax.set_title('Windspeed_Humidity outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Replace and impute the outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fancyimpute import KNN\n",
    "\n",
    "# create dataframe for outliers\n",
    "outliers = pd.DataFrame(df_all, columns=['windspeed','humidity'])\n",
    "\n",
    "# replace outliers by n/a\n",
    "columns = ['windspeed','humidity']\n",
    "for i in columns:\n",
    "    q75, q25 = np.percentile(outliers.loc[:,i],[75,25]) # Split data in 2 diff quantiles\n",
    "    iqr = q75 - q25 # inter quantile range\n",
    "    min = q25 - (iqr*1.5)\n",
    "    max = q75 + (iqr*1.5) \n",
    "    outliers.loc[outliers.loc[:,i] < min, :i] = np.nan\n",
    "    outliers.loc[outliers.loc[:,i] > max, :i] = np.nan\n",
    "\n",
    "# impute outliers by using the average\n",
    "outliers['windspeed'] = outliers['windspeed'].fillna(outliers['windspeed'].mean())\n",
    "outliers['humidity'] = outliers['humidity'].fillna(outliers['humidity'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Replace the original dataset by imputated dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the imputated windspeed\n",
    "df_all['windspeed'] = df_all['windspeed'].replace(outliers['windspeed'])\n",
    "\n",
    "#Replacing the imputated humidity\n",
    "df_all['humidity'] = df_all['humidity'].replace(outliers['humidity'])\n",
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Normal Probability Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "stats.probplot(df_all.total_count.tolist(), dist='norm',plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using Pearson Correlation\n",
    "df = df_all[df_all.columns]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments / reasoning:**\n",
    "\n",
    "- Regarding first total_count box plot, I observe no outliers in total bike rentals in this dataset\n",
    "- Regarding windspeed & humidity box plot analysis, I only observe outliers in windspeed and humidity features in this dataset\n",
    "- Regarding probability plot, there are few target variable data points that deviates from normality \n",
    "- Regarding the correlation matrix, I observe a significant positive correlation between season_fall and feel_temperature and also the target variable with feel_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_type = pd.get_dummies(df_all['season'], drop_first = True)\n",
    "season_type.rename(columns={2:\"season_summer\", 3:\"season_fall\", 4:\"season_winter\"},inplace=True)\n",
    "season_type.head()\n",
    "\n",
    "weather_type = pd.get_dummies(df_all['weather_condition'], drop_first = True)\n",
    "weather_type.rename(columns={2:\"weather_mist_cloud\", 3:\"weather_light_snow_rain\"},inplace=True)\n",
    "weather_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate new dummy variables to df_all\n",
    "df_all = pd.concat([df_all, season_type, weather_type], axis = 1)\n",
    "\n",
    "# drop previous columns season & weathersit\n",
    "df_all.drop(columns=[\"season\", \"weather_condition\"],axis=1, inplace =True)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#436EEE\"> Questions and Answers\n",
    "2. Visualise rentals of bikes per day. \n",
    "3. Assume that each bike has exactly maximum 12 rentals per day.\n",
    "    * Find the maximum number of bicycles `nmax` that was needed in any one day.\n",
    "    * Find the 95%-percentile of bicycles `n95` that was needed in any one day. \n",
    "4. Visualize the distribution of the covered days depending on the number of available bicycles (e.g. `nmax` bicycles would cover 100% of days, `n95` covers 95%, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate rentals of bikes per day of the week\n",
    "total_rents_by_day = df_all[['datetime', 'total_count']]\n",
    "#total_rents_by_day\n",
    "\n",
    "# visualize data\n",
    "plt.figure(figsize = (30, 15))\n",
    "\n",
    "fig = px.line(total_rents_by_day, x = 'datetime', y = 'total_count', title = 'Total Rentals per Day')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Max Number of Bikes = Total requested riders / Max no of rides per bike\n",
    "# Max Number of Bikes = total_count / 12\n",
    "\n",
    "df_all[\"total_count_max12\"] = df_all[\"total_count\"]/12\n",
    "#df_all.head()\n",
    "\n",
    "# calculate the maximum number of bicycles nmax that was needed in any one day\n",
    "nmax = pd.Series(df_all[\"total_count_max12\"])\n",
    "print(\"The maximum number of bicycles nmax that was needed in any one day is\", round(nmax.quantile(1, 'nearest'), 1), \"!\")\n",
    "\n",
    "# calculate the 95%-percentile of bicycles n95 that was needed in any one day\n",
    "n95 = pd.Series(df_all[\"total_count_max12\"])\n",
    "print(\"The 95%-percentile of bicycles n95 that was needed in any one day is\", round(nmax.quantile(0.95, 'nearest'), 1), \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = list(range(1,101))\n",
    "b = [scoreatpercentile(df_all[\"total_count\"],i) for i in a]\n",
    "\n",
    "df2 = pd.DataFrame({'percentile': a, 'total_count': b}, columns=['percentile', 'total_count'])\n",
    "fig = px.line(df2, x = 'percentile', y = 'total_count', title = 'Distribution of the Covered Days Depending on the Number of Available Bicycles')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Building prediction models\n",
    "\n",
    "**Tasks:**\n",
    "1. Define a test metric for predicting the daily demand for bike sharing, which you would like to use to measure the accuracy of the constructed models, and explain your choice.\n",
    "2. Build a demand prediction model with Random Forest, preferably making use of following python libraries: scikit-learn. \n",
    "3. Report the value of the chosen test metric on the provided data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Define Test Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bike sharing demand prediction refers to the process of forecasting the number of bicycles that will be rented within a specific time period, aiding in resource allocation and system optimization.\n",
    "For predicting the daily demand for bike sharing, which is a regression model, since the target variable is a quantity (over time) and consequent model evaluation to access the performance of this forecasting model, I'll use various metrics to evaluate the performance:\n",
    "- mean absolute error (MAE)\n",
    "- root mean squared error (RMSE)\n",
    "- coefficient of determination (R-squared).\n",
    "\n",
    "MAE and RMSE measure the average magnitude of the errors between the predicted and actual values. \\\n",
    "R-squared measures the proportion of variance in the target variable, explained by the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Train a Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next step is to train a Regression Model (in this case Random Forest), which will use the potentially predictive features we have identified to forecast the “total_count” label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define training dataset\n",
    "df = df_all.iloc[:-30, :]\n",
    "\n",
    "df.columns\n",
    "df.dtypes\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Set target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump not needed columns\n",
    "training_data = df.drop(['datetime', 'total_count_max12'], axis=1)\n",
    "\n",
    "# move total_count as last column\n",
    "training_data = training_data[ [ col for col in training_data.columns if col != 'total_count' ] + ['total_count']]\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Split into train and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the dataset into the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data.iloc[:,0:-1], training_data.iloc[:,-1], test_size = 0.2, random_state = 0)\n",
    "\n",
    "print('x train :', X_train.shape,'\\t\\tx test :', X_test.shape)\n",
    "print('y train :', y_train.shape,'\\t\\ty test :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Split the features into categorical and numerical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new dataset for train attributes\n",
    "train_attributes = X_train[X_train.columns]\n",
    "\n",
    "# create a new dataset for test attributes\n",
    "test_attributes = X_test[X_test.columns]\n",
    "\n",
    "# split dataframe by numerical and categorical columns\n",
    "num_cols = X_train.select_dtypes(include = ['uint8', 'int64', 'float64']).columns\n",
    "cat_cols = X_train.select_dtypes(include = ['object', 'bool', 'category']).columns\n",
    "\n",
    "print(\"There are {} numeric columns and {} categorical columns\".format(len(num_cols), len(cat_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Decoding the training attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get dummy variables to encode the categorical features to numeric\n",
    "train_encoded_attributes = pd.get_dummies(train_attributes, columns = cat_cols)\n",
    "\n",
    "print('Shape of transfomed dataframe:', train_encoded_attributes.shape)\n",
    "train_encoded_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Decoding the training attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training dataset for modelling\n",
    "X_train = train_encoded_attributes\n",
    "y_train = y_train #.total_count.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "X_train = train_encoded_attributes\n",
    "model = RandomForestRegressor(random_state = 0, n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the trained model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Cross validation prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is used to estimate the performance of machine learning models, more specificaly, it is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict = cross_val_predict(model, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross validation prediction plot\n",
    "fig,ax = plt.subplots(figsize=(15,8))\n",
    "ax.scatter(y_train, y_train-predict)\n",
    "ax.axhline(lw=2,color='black')\n",
    "ax.set_title('Cross validation prediction plot')\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Residual')\n",
    "#plt.show()\n",
    "\n",
    "# calculate equation for trendline\n",
    "z = np.polyfit(y_train, y_train-predict, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# add trendline to plot\n",
    "plt.plot(y_train, p(y_train), color=\"lightgreen\", linewidth=3, linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# R-squared scores\n",
    "r2_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print('R^2 scores :', np.average(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers / comments / reasoning:**\n",
    "\n",
    "- Observing cross validation prediction plot we see there is an apparent diagonal trend and the points where the predicted and actual values intersect generally follow the trend line. There is a good fitness of the model in this case, however it some data points present a higher variation. Normally if there's a variation, it represents the model’s residuals, which are the differences between the predicted label and the actual value of the validation label when the model applies the coefficients it learned during training to the validation data. By assessing these residuals from the validation data, we can estimate the level of error that can be expected when the model is used with new data for which the label is unknown.\n",
    "- The R-squared or coefficient of determination is ~ 85.7% on average for 5-fold cross validation, it means that predictor is only able to predict 85.7% of the variance in the target variable which is contributed by independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Decoding the test attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get dummy variables to encode the categorical features to numeric\n",
    "test_encoded_attributes=pd.get_dummies(test_attributes,columns=cat_cols)\n",
    "\n",
    "print('Shape of transformed dataframe :', test_encoded_attributes.shape)\n",
    "test_encoded_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Model performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict the model\n",
    "X_test = test_encoded_attributes\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# R-squared scores\n",
    "r2_scores = cross_val_score(model, X_test, y_test, cv=5)\n",
    "print('R^2 scores :', np.average(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find best value for n_estimators\n",
    "max = 0\n",
    "index = -1\n",
    "for i in range(10, 200):\n",
    "    model = RandomForestRegressor(random_state = 0, n_estimators = i)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2_score = metrics.r2_score(y_test, y_pred)\n",
    "    if r2_score > max:\n",
    "        index = i\n",
    "        max = r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confirm same dimension for the target variable\n",
    "y_test.shape\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_opt = RandomForestRegressor(random_state = 0, n_estimators = index)\n",
    "model_opt.fit(X_train, y_train)\n",
    "y_pred = model_opt.predict(X_test)\n",
    "\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "me = metrics.max_error(y_test, y_pred)\n",
    "\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Max Error:', me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# R-squared scores\n",
    "r2_scores = cross_val_score(model_opt, X_test, y_test, cv=5)\n",
    "print('R^2 scores :', np.average(r2_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers / comments / reasoning:**\n",
    "\n",
    "- After parameter tuning, the R-squared or coefficient of determination is the same (~80.3%), it means that this optimization did not enhance the results of the model, probably because we have limited and small dataset and also the difference in n_estimators was less than 3%. It would at least optimize the running time if dataset were bigger and less balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the optimized trained model as a pickle file\n",
    "saved_model = pickle.dumps('model/model_opt')\n",
    "  \n",
    "# Load the pickled model\n",
    "rf_model_pkl = pickle.loads('model/saved_model')\n",
    "  \n",
    "# Use the loaded pickled model to make predictions\n",
    "#rf_model_pkl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Residual plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# residual scatter plot\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "residuals=y_test-y_pred\n",
    "ax.scatter(y_test, residuals)\n",
    "ax.axhline(lw=2, color='black')\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residual plot')\n",
    "#plt.show()\n",
    "\n",
    "# calculate equation for trendline\n",
    "z = np.polyfit(y_test, residuals, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# add trendline to plot\n",
    "plt.plot(y_test, p(y_test), color=\"lightgreen\", linewidth=3, linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Predicting Bike Rental count on Daily basis, in Out-of-Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-sample testing is used to evaluate the performance of a strategy on a separate set of data that was not used during the development and optimisation process. \\\n",
    "This helps to determine whether the strategy would be able to perform well on new, unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define out-of-sample dataset\n",
    "df_last30 = df_all.tail(30)\n",
    "df_last30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save date variable\n",
    "times = df_last30['datetime']\n",
    "\n",
    "# dump not needed columns\n",
    "testing_data = df_last30.drop(['datetime', 'total_count_max12'], axis=1)\n",
    "\n",
    "# move total_count as last column\n",
    "testing_data = testing_data[ [ col for col in testing_data.columns if col != 'total_count' ] + ['total_count']]\n",
    "testing_data.head()\n",
    "print('Shape of OOS dataframe :', testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new dataset for test attributes\n",
    "testing_data_attributes = testing_data[testing_data.columns]\n",
    "\n",
    "# split dataframe by numerical and categorical columns\n",
    "num_cols = testing_data.select_dtypes(include = ['uint8', 'int64', 'float64']).columns\n",
    "cat_cols = testing_data.select_dtypes(include = ['object', 'bool', 'category']).columns\n",
    "\n",
    "print(\"There are {} numeric columns and {} categorical columns\".format(len(num_cols), len(cat_cols)))\n",
    "\n",
    "# get dummy variables to encode the categorical features to numeric\n",
    "testing_data_encoded_attributes = pd.get_dummies(testing_data_attributes, columns=cat_cols)\n",
    "\n",
    "# drop target variable\n",
    "testing_data_encoded_attributes = testing_data_encoded_attributes.drop(['total_count'], axis = 1)\n",
    "\n",
    "print('Shape of transformed dataframe :', testing_data_encoded_attributes.shape)\n",
    "testing_data_encoded_attributes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_testing = model_opt.predict(testing_data_encoded_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit final sample\n",
    "Submission = pd.DataFrame({'datetime' : times, 'pred' : y_pred_testing})\n",
    "Submission.set_index('datetime', inplace = True)\n",
    "Submission.to_csv('output/sample_submission.csv')\n",
    "Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#436EEE\"> Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Reflection / comments\n",
    "\n",
    "**Tasks:**\n",
    "(Optional) Please share with us any free form reflection, comments or feedback you have in the context of this test task.\n",
    "\n",
    "In summary, this notebook conducted a comprehensive analysis of daily bike rental data in two years time basis. Hands-on in data exploration, preprocessing, and feature engineering to prepare the data for modeling. The exploratory data analysis provided valuable insights into rental patterns based on different factors, such as weather, day of the week, seasonality, etc.\n",
    "In conclusion, this notebook gave us great insights on bike rental trends and successfully predicted rental counts using the Random Forest model.\n",
    "\n",
    "Future improvements:\n",
    "1. Exploratory Data Analysis using hour dataset\n",
    "2. More feature engineering using day/hour dataset features\n",
    "3. Tuning other parameters inside RF\n",
    "4. Apply other models to compare performances\n",
    "5. Employing advanced feature selection/explainability techniques (eg. SHAP)\n",
    "\n",
    "The analysis and insights presented here can provide valuable guidance for bike-sharing companies to optimize their services and meet the diverse preferences of their user base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Please submit this notebook with your developments in .ipynb and .html formats as well as your requirements.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalab-task",
   "language": "python",
   "name": "datalab-task"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
